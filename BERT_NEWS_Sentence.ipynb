{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForPreTraining,BertForTokenClassification ,Trainer, TrainingArguments\n",
    "import os\n",
    "import torch \n",
    "import pandas as pd\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from array import array\n",
    "import numpy as np\n",
    "def loadData():\n",
    "    list_subfolders_with_paths = [f.path for f in os.scandir(\"./annotation_sentence\") if f.is_dir()]\n",
    "    data = {\"texts\": [],\"labels\":[]}\n",
    "    for i in list_subfolders_with_paths:\n",
    "    #print(\"new File\")\n",
    "        texts=[]\n",
    "        labels=[]\n",
    "        with open(f\"{i}/admin.conll\",\"r\", encoding=\"utf-8-sig\") as file: \n",
    "            for j in  file.readlines():\n",
    "                if(j.strip() == \"\"):\n",
    "                    continue\n",
    "                sp = j.split(\" \")\n",
    "                if(\"null\" in sp[1].rstrip(\"\\n\")):\n",
    "                    print(i)\n",
    "                texts.append(sp[0])\n",
    "                labels.append(sp[1].rstrip(\"\\n\"))\n",
    "                #print(f\"Text: {sp[0]}\")\n",
    "                #print(f\"Label: {sp[1]}\")\n",
    "            data[\"texts\"].append((texts))\n",
    "            data[\"labels\"].append(( labels))\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset , ClassLabel ,Features ,Sequence ,Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = loadData()\n",
    "texts = traindata[\"texts\"]\n",
    "tags = traindata[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set(tag for doc in tags for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-SENTENCE', 'O', 'B-SENTENCE']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(loadData(),features=Features( {\"texts\": Sequence(Value(\"string\")),'labels':Sequence(ClassLabel(names=list(unique_tags),num_classes =len(unique_tags)))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I-SENTENCE': 0, 'O': 1, 'B-SENTENCE': 2}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset.features[\"labels\"].feature)._str2int \n",
    "#{str(v): k for k, v in (dataset.features[\"labels\"].feature)._str2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 9.13, 'B-SENTENCE': 6.05, 'I-SENTENCE': 84.82}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "x = [i if(len(i) > 1) else i for i in list(sum(tags, []))]\n",
    "di = dict(Counter(x))\n",
    "z = {}\n",
    "\n",
    "for i in di.keys():\n",
    "\n",
    "    z[i] = round ((di[i]/(len(x)))*100,2)\n",
    "    \n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(num_classes=3, names=['I-SENTENCE', 'O', 'B-SENTENCE'], id=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset[\"train\"].features[\"labels\"].feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'texts': ['BETRUG',\n",
       "  'MIT',\n",
       "  'POLIZISTEN-TRICK',\n",
       "  'Bankerin',\n",
       "  'rettet',\n",
       "  'für',\n",
       "  'Seniorin',\n",
       "  '12',\n",
       "  '000',\n",
       "  'Euro',\n",
       "  'Die',\n",
       "  'Seniorin',\n",
       "  'wollte',\n",
       "  '12.000',\n",
       "  'Euro',\n",
       "  'abheben',\n",
       "  '-',\n",
       "  'eine',\n",
       "  'Bankangestellte',\n",
       "  'verhinderte',\n",
       "  'das',\n",
       "  '(',\n",
       "  'Symbolbild',\n",
       "  ')',\n",
       "  'Foto',\n",
       "  ':',\n",
       "  'picture',\n",
       "  'alliance',\n",
       "  '/',\n",
       "  'VisualEyze',\n",
       "  '18.08.2021',\n",
       "  '-',\n",
       "  '15',\n",
       "  ':',\n",
       "  '20',\n",
       "  'Uhr',\n",
       "  'Kassel',\n",
       "  '–',\n",
       "  'Zum',\n",
       "  'Glück',\n",
       "  'hat',\n",
       "  'hier',\n",
       "  'eine',\n",
       "  'Bankangestellte',\n",
       "  'aufgepasst',\n",
       "  '!',\n",
       "  'Ihr',\n",
       "  'beherztes',\n",
       "  'Eingreifen',\n",
       "  'hat',\n",
       "  'verhindert',\n",
       "  ',',\n",
       "  'dass',\n",
       "  'eine',\n",
       "  'Seniorin',\n",
       "  '(',\n",
       "  '79',\n",
       "  ')',\n",
       "  'um',\n",
       "  '12',\n",
       "  '000',\n",
       "  'Euro',\n",
       "  'betrogen',\n",
       "  'wurde',\n",
       "  '.',\n",
       "  'Der',\n",
       "  'Angestellten',\n",
       "  'kam',\n",
       "  'es',\n",
       "  'komisch',\n",
       "  'vor',\n",
       "  ',',\n",
       "  'dass',\n",
       "  'die',\n",
       "  'Seniorin',\n",
       "  'am',\n",
       "  'Dienstagmittag',\n",
       "  'einen',\n",
       "  'so',\n",
       "  'hohen',\n",
       "  'Geldbetrag',\n",
       "  'abheben',\n",
       "  'wollte',\n",
       "  '.',\n",
       "  'Als',\n",
       "  'die',\n",
       "  'aufmerksame',\n",
       "  'Mitarbeiterin',\n",
       "  'der',\n",
       "  'Kasseler',\n",
       "  'Bankfiliale',\n",
       "  'die',\n",
       "  '79',\n",
       "  '-',\n",
       "  'Jährige',\n",
       "  'nach',\n",
       "  'dem',\n",
       "  'Grund',\n",
       "  'für',\n",
       "  'die',\n",
       "  'Abhebung',\n",
       "  'fragte',\n",
       "  ',',\n",
       "  'stellte',\n",
       "  'sich',\n",
       "  'heraus',\n",
       "  ',',\n",
       "  'dass',\n",
       "  'diese',\n",
       "  'zuvor',\n",
       "  'von',\n",
       "  'Betrügern',\n",
       "  'angerufen',\n",
       "  'worden',\n",
       "  'war',\n",
       "  '.',\n",
       "  'Die',\n",
       "  'miese',\n",
       "  'Betrugs-Masche',\n",
       "  ':',\n",
       "  'Der',\n",
       "  'Anrufer',\n",
       "  'hatte',\n",
       "  'sich',\n",
       "  'als',\n",
       "  'Polizist',\n",
       "  'ausgegeben',\n",
       "  '.',\n",
       "  'Er',\n",
       "  'behauptete',\n",
       "  ',',\n",
       "  'dass',\n",
       "  'die',\n",
       "  'Tochter',\n",
       "  'der',\n",
       "  'Frau',\n",
       "  'einen',\n",
       "  'schweren',\n",
       "  'Autounfall',\n",
       "  'verursacht',\n",
       "  'hat',\n",
       "  '.',\n",
       "  'Und',\n",
       "  'einer',\n",
       "  'Gefängnisstrafe',\n",
       "  'nur',\n",
       "  'entgehen',\n",
       "  'kann',\n",
       "  ',',\n",
       "  'wenn',\n",
       "  'sie',\n",
       "  'mehrere',\n",
       "  'Tausend',\n",
       "  'Euro',\n",
       "  'Kaution',\n",
       "  'zahlt',\n",
       "  '.',\n",
       "  'Die',\n",
       "  'Täter',\n",
       "  'hatten',\n",
       "  'der',\n",
       "  'Seniorin',\n",
       "  'sogar',\n",
       "  'ein',\n",
       "  'Taxi',\n",
       "  'bestellt',\n",
       "  ',',\n",
       "  'das',\n",
       "  'sie',\n",
       "  'zur',\n",
       "  'Bank',\n",
       "  'brachte',\n",
       "  '!',\n",
       "  'Die',\n",
       "  'Ermittlungen',\n",
       "  'dauern',\n",
       "  'an',\n",
       "  '.',\n",
       "  'Quelle',\n",
       "  ':',\n",
       "  'info.bild.deEnkeltrick-Fälle',\n",
       "  'in',\n",
       "  'DeutschlandFallzahlen',\n",
       "  'aller',\n",
       "  'Landeskriminalämter',\n",
       "  '*',\n",
       "  'Angaben',\n",
       "  'für',\n",
       "  'das',\n",
       "  'Jahr',\n",
       "  '2015',\n",
       "  ',',\n",
       "  'weil',\n",
       "  'keine',\n",
       "  'aktuelleren',\n",
       "  'Zahlen',\n",
       "  'vorlagen',\n",
       "  'info.BILD.de',\n",
       "  '|',\n",
       "  'Quelle',\n",
       "  ':',\n",
       "  'Landeskriminalämter',\n",
       "  'der',\n",
       "  'Länder',\n",
       "  '|',\n",
       "  'Stand',\n",
       "  ':',\n",
       "  'November',\n",
       "  '2019'],\n",
       " 'labels': [1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = dataset[\"train\"].features[f\"labels\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/gbert-large were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at deepset/gbert-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Load Model and Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('deepset/gbert-large')\n",
    "model = BertForTokenClassification.from_pretrained('deepset/gbert-large', num_labels=len(label_names) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.label2id =  (dataset[\"train\"].features[\"labels\"].feature)._str2int \n",
    "model.config.id2label =  {str(v): k for k, v in (dataset[\"train\"].features[\"labels\"].feature)._str2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    #print(examples.keys())\n",
    "    tokenized_inputs = tokenizer(examples[\"texts\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9228c4f4d93e409e90096e3b9c1ec40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6a34f2d0734f94a87f0af8006e970c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(examples):\n",
    "    dic ={\"input_ids\":[],\"token_type_ids\":[],\"attention_mask\":[],\"labels\":[]}\n",
    "    for j in examples:\n",
    "        if j == \"texts\":\n",
    "            continue\n",
    "        for i in range(len(examples[\"input_ids\"])):\n",
    "            arr = np.array_split(examples[j][i],2)\n",
    "            dic[j].append(arr[0])\n",
    "            dic[j].append(arr[1])\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49e30c0be0b4ef2b7234414c9a261d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5143059195f438b83b53b792f9a83c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = tokenized_datasets.map(split,remove_columns=[\"input_ids\",\"token_type_ids\",\"attention_mask\",\"labels\",\"texts\"], batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset = test[\"train\"]\n",
    "full_eval_dataset = test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    for k in results.keys():\n",
    "      if(k not in flattened_results.keys()):\n",
    "        flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
    "\n",
    "    return flattened_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=6,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_args, train_dataset=full_train_dataset, eval_dataset=full_eval_dataset,compute_metrics=compute_metrics, data_collator=data_collator,\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 78\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 6\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1300' max='1300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1300/1300 05:51, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>Sentence F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.547122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.845537</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.527561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.845537</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.433855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.845537</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.383919</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.903841</td>\n",
       "      <td>0.028986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.141118</td>\n",
       "      <td>0.350649</td>\n",
       "      <td>0.248848</td>\n",
       "      <td>0.291105</td>\n",
       "      <td>0.946947</td>\n",
       "      <td>0.291105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.096307</td>\n",
       "      <td>0.579545</td>\n",
       "      <td>0.705069</td>\n",
       "      <td>0.636175</td>\n",
       "      <td>0.967671</td>\n",
       "      <td>0.636175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.095735</td>\n",
       "      <td>0.613757</td>\n",
       "      <td>0.534562</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.963526</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.065129</td>\n",
       "      <td>0.757848</td>\n",
       "      <td>0.778802</td>\n",
       "      <td>0.768182</td>\n",
       "      <td>0.978447</td>\n",
       "      <td>0.768182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.075462</td>\n",
       "      <td>0.719512</td>\n",
       "      <td>0.815668</td>\n",
       "      <td>0.764579</td>\n",
       "      <td>0.976789</td>\n",
       "      <td>0.764579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.066883</td>\n",
       "      <td>0.772321</td>\n",
       "      <td>0.797235</td>\n",
       "      <td>0.784580</td>\n",
       "      <td>0.979000</td>\n",
       "      <td>0.784580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.072097</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.760181</td>\n",
       "      <td>0.977618</td>\n",
       "      <td>0.760181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.098389</td>\n",
       "      <td>0.758475</td>\n",
       "      <td>0.824885</td>\n",
       "      <td>0.790287</td>\n",
       "      <td>0.975131</td>\n",
       "      <td>0.790287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.090462</td>\n",
       "      <td>0.782979</td>\n",
       "      <td>0.847926</td>\n",
       "      <td>0.814159</td>\n",
       "      <td>0.978447</td>\n",
       "      <td>0.814159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.084289</td>\n",
       "      <td>0.779736</td>\n",
       "      <td>0.815668</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.980934</td>\n",
       "      <td>0.797297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.085750</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.980381</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.093649</td>\n",
       "      <td>0.780591</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.814978</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.814978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.071660</td>\n",
       "      <td>0.767932</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.801762</td>\n",
       "      <td>0.980381</td>\n",
       "      <td>0.801762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.092712</td>\n",
       "      <td>0.748918</td>\n",
       "      <td>0.797235</td>\n",
       "      <td>0.772321</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.772321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.070128</td>\n",
       "      <td>0.762295</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.806941</td>\n",
       "      <td>0.980658</td>\n",
       "      <td>0.806941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.080518</td>\n",
       "      <td>0.811927</td>\n",
       "      <td>0.815668</td>\n",
       "      <td>0.813793</td>\n",
       "      <td>0.982039</td>\n",
       "      <td>0.813793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.090467</td>\n",
       "      <td>0.824786</td>\n",
       "      <td>0.889401</td>\n",
       "      <td>0.855876</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.855876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.096655</td>\n",
       "      <td>0.825328</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.847534</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>0.847534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.104575</td>\n",
       "      <td>0.786611</td>\n",
       "      <td>0.866359</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.981487</td>\n",
       "      <td>0.824561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.094638</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.875576</td>\n",
       "      <td>0.840708</td>\n",
       "      <td>0.982039</td>\n",
       "      <td>0.840708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114097</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.746544</td>\n",
       "      <td>0.758782</td>\n",
       "      <td>0.978171</td>\n",
       "      <td>0.758782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.116536</td>\n",
       "      <td>0.821277</td>\n",
       "      <td>0.889401</td>\n",
       "      <td>0.853982</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.853982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.116677</td>\n",
       "      <td>0.823789</td>\n",
       "      <td>0.861751</td>\n",
       "      <td>0.842342</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.842342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.133217</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.861751</td>\n",
       "      <td>0.816594</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.816594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.130278</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.815668</td>\n",
       "      <td>0.806378</td>\n",
       "      <td>0.980934</td>\n",
       "      <td>0.806378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.110860</td>\n",
       "      <td>0.809735</td>\n",
       "      <td>0.843318</td>\n",
       "      <td>0.826185</td>\n",
       "      <td>0.981487</td>\n",
       "      <td>0.826185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.118693</td>\n",
       "      <td>0.791304</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.814318</td>\n",
       "      <td>0.981487</td>\n",
       "      <td>0.814318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.153621</td>\n",
       "      <td>0.819005</td>\n",
       "      <td>0.834101</td>\n",
       "      <td>0.826484</td>\n",
       "      <td>0.980381</td>\n",
       "      <td>0.826484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.121042</td>\n",
       "      <td>0.832599</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.983973</td>\n",
       "      <td>0.851351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.131399</td>\n",
       "      <td>0.838565</td>\n",
       "      <td>0.861751</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.123817</td>\n",
       "      <td>0.836283</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.853273</td>\n",
       "      <td>0.983145</td>\n",
       "      <td>0.853273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.128506</td>\n",
       "      <td>0.801724</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.828508</td>\n",
       "      <td>0.980934</td>\n",
       "      <td>0.828508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.137158</td>\n",
       "      <td>0.814159</td>\n",
       "      <td>0.847926</td>\n",
       "      <td>0.830700</td>\n",
       "      <td>0.981487</td>\n",
       "      <td>0.830700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.123844</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.875576</td>\n",
       "      <td>0.840708</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>0.840708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.119490</td>\n",
       "      <td>0.818584</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.835214</td>\n",
       "      <td>0.982868</td>\n",
       "      <td>0.835214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.128720</td>\n",
       "      <td>0.841629</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.849315</td>\n",
       "      <td>0.981763</td>\n",
       "      <td>0.849315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.126265</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.861751</td>\n",
       "      <td>0.853881</td>\n",
       "      <td>0.982039</td>\n",
       "      <td>0.853881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.116429</td>\n",
       "      <td>0.837719</td>\n",
       "      <td>0.880184</td>\n",
       "      <td>0.858427</td>\n",
       "      <td>0.983697</td>\n",
       "      <td>0.858427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>0.848889</td>\n",
       "      <td>0.880184</td>\n",
       "      <td>0.864253</td>\n",
       "      <td>0.983145</td>\n",
       "      <td>0.864253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.141398</td>\n",
       "      <td>0.832558</td>\n",
       "      <td>0.824885</td>\n",
       "      <td>0.828704</td>\n",
       "      <td>0.981763</td>\n",
       "      <td>0.828704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.130106</td>\n",
       "      <td>0.802632</td>\n",
       "      <td>0.843318</td>\n",
       "      <td>0.822472</td>\n",
       "      <td>0.979552</td>\n",
       "      <td>0.822472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.123811</td>\n",
       "      <td>0.845133</td>\n",
       "      <td>0.880184</td>\n",
       "      <td>0.862302</td>\n",
       "      <td>0.983973</td>\n",
       "      <td>0.862302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.126655</td>\n",
       "      <td>0.839450</td>\n",
       "      <td>0.843318</td>\n",
       "      <td>0.841379</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>0.841379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.112709</td>\n",
       "      <td>0.839450</td>\n",
       "      <td>0.843318</td>\n",
       "      <td>0.841379</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>0.841379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.100762</td>\n",
       "      <td>0.822727</td>\n",
       "      <td>0.834101</td>\n",
       "      <td>0.828375</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>0.828375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.105822</td>\n",
       "      <td>0.840183</td>\n",
       "      <td>0.847926</td>\n",
       "      <td>0.844037</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.844037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.114339</td>\n",
       "      <td>0.831818</td>\n",
       "      <td>0.843318</td>\n",
       "      <td>0.837529</td>\n",
       "      <td>0.982039</td>\n",
       "      <td>0.837529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.098031</td>\n",
       "      <td>0.788136</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.821192</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>0.821192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.116660</td>\n",
       "      <td>0.825112</td>\n",
       "      <td>0.847926</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.836364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.103178</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.827740</td>\n",
       "      <td>0.982039</td>\n",
       "      <td>0.827740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.086572</td>\n",
       "      <td>0.792208</td>\n",
       "      <td>0.843318</td>\n",
       "      <td>0.816964</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.816964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.089834</td>\n",
       "      <td>0.808889</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.101592</td>\n",
       "      <td>0.830275</td>\n",
       "      <td>0.834101</td>\n",
       "      <td>0.832184</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.832184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.099407</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.843318</td>\n",
       "      <td>0.833713</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.833713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.098089</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.843318</td>\n",
       "      <td>0.833713</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>0.833713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.108196</td>\n",
       "      <td>0.851163</td>\n",
       "      <td>0.843318</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>0.983145</td>\n",
       "      <td>0.847222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.113776</td>\n",
       "      <td>0.851163</td>\n",
       "      <td>0.843318</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>0.983145</td>\n",
       "      <td>0.847222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.112148</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>0.843318</td>\n",
       "      <td>0.845266</td>\n",
       "      <td>0.982868</td>\n",
       "      <td>0.845266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.113581</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.829493</td>\n",
       "      <td>0.823799</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.823799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.109769</td>\n",
       "      <td>0.853881</td>\n",
       "      <td>0.861751</td>\n",
       "      <td>0.857798</td>\n",
       "      <td>0.983421</td>\n",
       "      <td>0.857798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.111394</td>\n",
       "      <td>0.837104</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.844749</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>0.844749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.113606</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.842825</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.842825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.116975</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.843318</td>\n",
       "      <td>0.828054</td>\n",
       "      <td>0.981487</td>\n",
       "      <td>0.828054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.115849</td>\n",
       "      <td>0.829596</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.982039</td>\n",
       "      <td>0.840909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.115528</td>\n",
       "      <td>0.829596</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.982039</td>\n",
       "      <td>0.840909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.117276</td>\n",
       "      <td>0.829596</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.982039</td>\n",
       "      <td>0.840909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.117904</td>\n",
       "      <td>0.825893</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.839002</td>\n",
       "      <td>0.981763</td>\n",
       "      <td>0.839002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.118275</td>\n",
       "      <td>0.825893</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.839002</td>\n",
       "      <td>0.982039</td>\n",
       "      <td>0.839002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.121714</td>\n",
       "      <td>0.829596</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.982039</td>\n",
       "      <td>0.840909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.123304</td>\n",
       "      <td>0.829596</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.982039</td>\n",
       "      <td>0.840909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.124258</td>\n",
       "      <td>0.829596</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.982039</td>\n",
       "      <td>0.840909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.124891</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.842825</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.842825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.126109</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.842825</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.842825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.128256</td>\n",
       "      <td>0.832579</td>\n",
       "      <td>0.847926</td>\n",
       "      <td>0.840183</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.840183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.128549</td>\n",
       "      <td>0.832579</td>\n",
       "      <td>0.847926</td>\n",
       "      <td>0.840183</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.840183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.136189</td>\n",
       "      <td>0.791111</td>\n",
       "      <td>0.820276</td>\n",
       "      <td>0.805430</td>\n",
       "      <td>0.980658</td>\n",
       "      <td>0.805430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.129686</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.829493</td>\n",
       "      <td>0.820046</td>\n",
       "      <td>0.981487</td>\n",
       "      <td>0.820046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.124639</td>\n",
       "      <td>0.840183</td>\n",
       "      <td>0.847926</td>\n",
       "      <td>0.844037</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>0.844037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.123156</td>\n",
       "      <td>0.840183</td>\n",
       "      <td>0.847926</td>\n",
       "      <td>0.844037</td>\n",
       "      <td>0.982868</td>\n",
       "      <td>0.844037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.123044</td>\n",
       "      <td>0.840183</td>\n",
       "      <td>0.847926</td>\n",
       "      <td>0.844037</td>\n",
       "      <td>0.982868</td>\n",
       "      <td>0.844037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.122963</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.847926</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.123412</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.847926</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.126467</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>0.838710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.127160</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>0.838710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.127246</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.829493</td>\n",
       "      <td>0.831409</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.831409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.127464</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>0.838710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.125852</td>\n",
       "      <td>0.828829</td>\n",
       "      <td>0.847926</td>\n",
       "      <td>0.838269</td>\n",
       "      <td>0.982039</td>\n",
       "      <td>0.838269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.122912</td>\n",
       "      <td>0.820628</td>\n",
       "      <td>0.843318</td>\n",
       "      <td>0.831818</td>\n",
       "      <td>0.981763</td>\n",
       "      <td>0.831818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.120451</td>\n",
       "      <td>0.825893</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.839002</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.839002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.120467</td>\n",
       "      <td>0.825893</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.839002</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.839002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.120659</td>\n",
       "      <td>0.825893</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.839002</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.839002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.120957</td>\n",
       "      <td>0.825893</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.839002</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.839002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.121567</td>\n",
       "      <td>0.834081</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.845455</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>0.845455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.122324</td>\n",
       "      <td>0.834081</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.845455</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.845455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.122351</td>\n",
       "      <td>0.834081</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.845455</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.845455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.122369</td>\n",
       "      <td>0.834081</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.845455</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>0.845455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1300, training_loss=0.029802646596844378, metrics={'train_runtime': 353.991, 'train_samples_per_second': 22.034, 'train_steps_per_second': 3.672, 'total_flos': 3596883492492180.0, 'train_loss': 0.029802646596844378, 'epoch': 100.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.12236879020929337,\n",
       " 'eval_overall_precision': 0.8340807174887892,\n",
       " 'eval_overall_recall': 0.8571428571428571,\n",
       " 'eval_overall_f1': 0.8454545454545455,\n",
       " 'eval_overall_accuracy': 0.9823155567836419,\n",
       " 'eval_SENTENCE_f1': 0.8454545454545455,\n",
       " 'eval_runtime': 0.2854,\n",
       " 'eval_samples_per_second': 70.084,\n",
       " 'eval_steps_per_second': 14.017,\n",
       " 'epoch': 100.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./NEWS_Sentence_2/config.json\n",
      "Model weights saved in ./NEWS_Sentence_2/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('./NEWS_Sentence_2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./NEWS_TOK_2/tokenizer_config.json\n",
      "Special tokens file saved in ./NEWS_TOK_2/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./NEWS_TOK_2/tokenizer_config.json',\n",
       " './NEWS_TOK_2/special_tokens_map.json',\n",
       " './NEWS_TOK_2/vocab.txt',\n",
       " './NEWS_TOK_2/added_tokens.json',\n",
       " './NEWS_TOK_2/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./NEWS_TOK_2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

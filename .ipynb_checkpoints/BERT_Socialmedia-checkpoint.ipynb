{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForPreTraining,BertForTokenClassification ,Trainer, TrainingArguments\n",
    "import os\n",
    "import torch \n",
    "import pandas as pd\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from array import array\n",
    "import numpy as np\n",
    "def loadData():\n",
    "    list_subfolders_with_paths = [f.path for f in os.scandir(\"./annotation_Facebook/\") if f.is_dir()]\n",
    "    data = {\"texts\": [],\"labels\":[]}\n",
    "    for i in list_subfolders_with_paths:\n",
    "    #print(\"new File\")\n",
    "        texts=[]\n",
    "        labels=[]\n",
    "        with open(f\"{i}/admin.conll\",\"r\", encoding=\"utf-8-sig\") as file: \n",
    "            for j in  file.readlines():\n",
    "                if(j.strip() == \"\"):\n",
    "                    continue\n",
    "                sp = j.split(\" \")\n",
    "                if(\"null\" in sp[1].rstrip(\"\\n\") or \"\\xa0\" in sp[1].rstrip(\"\\n\")):\n",
    "                    print(i)\n",
    "                texts.append(sp[0])\n",
    "                if (sp[1].rstrip(\"\\n\") == \"B-LOCATION\"):\n",
    "                    labels.append(\"B-ORGANIZATION\")\n",
    "                elif(sp[1].rstrip(\"\\n\") == \"I-LOCATION\"):\n",
    "                    labels.append(\"I-ORGANIZATION\")\n",
    "                else:\n",
    "                    labels.append(sp[1].rstrip(\"\\n\"))\n",
    "                #print(f\"Text: {sp[0]}\")\n",
    "                #print(f\"Label: {sp[1]}\")\n",
    "            data[\"texts\"].append((texts))\n",
    "            data[\"labels\"].append(( labels))\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset , ClassLabel ,Features ,Sequence ,Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = loadData()\n",
    "texts = traindata[\"texts\"]\n",
    "tags = traindata[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PERSON_FIRSTNAME': 2.89,\n",
       " 'PERSON_LASTNAME': 2.51,\n",
       " 'ORGANIZATION': 5.94,\n",
       " 'LOCATION': 3.43,\n",
       " 'MISC_FRIENDS': 27.88,\n",
       " 'LOCATION_CITY': 4.72,\n",
       " 'LOCATION_COUNTRY': 0.84,\n",
       " 'MISC_LIKES': 4.49,\n",
       " 'MISC_POST': 31.38,\n",
       " 'MISC_COMMENTS': 15.92}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "x = [i[2:] if(len(i) > 1) else i for i in list(sum(tags, []))]\n",
    "di = dict(Counter(x))\n",
    "\n",
    "z = {}\n",
    "\n",
    "for i in di.keys():\n",
    "    if i == \"O\":\n",
    "        continue\n",
    "    z[i] = round ((di[i]/(len(x)-di[\"O\"]))*100,2)\n",
    "    \n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set(tag for doc in tags for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-LOCATION',\n",
       " 'B-PERSON_FIRSTNAME',\n",
       " 'I-MISC_POST',\n",
       " 'I-MISC_COMMENTS',\n",
       " 'I-ORGANIZATION',\n",
       " 'I-LOCATION_CITY',\n",
       " 'I-PERSON_FIRSTNAME',\n",
       " 'O',\n",
       " 'I-LOCATION_COUNTRY',\n",
       " 'I-MISC_FRIENDS',\n",
       " 'B-MISC_COMMENTS',\n",
       " 'B-MISC_FRIENDS',\n",
       " 'B-ORGANIZATION',\n",
       " 'B-MISC_LIKES',\n",
       " 'B-MISC_POST',\n",
       " 'B-LOCATION_COUNTRY',\n",
       " 'B-LOCATION_CITY',\n",
       " 'B-PERSON_LASTNAME',\n",
       " 'B-LOCATION']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(loadData(),features=Features( {\"texts\": Sequence(Value(\"string\")),'labels':Sequence(ClassLabel(names=list(unique_tags),num_classes =len(unique_tags)))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(num_classes=19, names=['I-LOCATION', 'B-PERSON_FIRSTNAME', 'I-MISC_POST', 'I-MISC_COMMENTS', 'I-ORGANIZATION', 'I-LOCATION_CITY', 'I-PERSON_FIRSTNAME', 'O', 'I-LOCATION_COUNTRY', 'I-MISC_FRIENDS', 'B-MISC_COMMENTS', 'B-MISC_FRIENDS', 'B-ORGANIZATION', 'B-MISC_LIKES', 'B-MISC_POST', 'B-LOCATION_COUNTRY', 'B-LOCATION_CITY', 'B-PERSON_LASTNAME', 'B-LOCATION'], names_file=None, id=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset[\"train\"].features[\"labels\"].feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'texts': ['Njomza',\n",
       "  'Krasniqi',\n",
       "  '100',\n",
       "  'Freunde',\n",
       "  'Freund',\n",
       "  '/',\n",
       "  'in',\n",
       "  'hinzufügen',\n",
       "  'Nachricht',\n",
       "  'senden',\n",
       "  'Steckbrief',\n",
       "  'Wohnt',\n",
       "  'in',\n",
       "  'Köln',\n",
       "  'Single',\n",
       "  'Fotos',\n",
       "  'Alle',\n",
       "  'Fotos',\n",
       "  'ansehen',\n",
       "  'Freunde',\n",
       "  'Alle',\n",
       "  'Freunde',\n",
       "  'ansehen',\n",
       "  '100',\n",
       "  'Freunde',\n",
       "  'Njomza',\n",
       "  'Krasniqi',\n",
       "  '\\xa0',\n",
       "  '1',\n",
       "  'Privatsphäre',\n",
       "  '·',\n",
       "  'Impressum',\n",
       "  '/',\n",
       "  'Terms',\n",
       "  '/',\n",
       "  'NetzDG',\n",
       "  '/',\n",
       "  'UrhDaG',\n",
       "  '·',\n",
       "  'Werbung',\n",
       "  '·',\n",
       "  'Datenschutzinfo',\n",
       "  '·',\n",
       "  'Cookies',\n",
       "  '·',\n",
       "  '·',\n",
       "  'Meta',\n",
       "  '©',\n",
       "  '2022',\n",
       "  'Arlind',\n",
       "  'Krasniqi',\n",
       "  'Salwan',\n",
       "  'Alqaidi',\n",
       "  'Agnesa',\n",
       "  'Sekiraqa',\n",
       "  'Daffina',\n",
       "  'Iimeri',\n",
       "  'Florian',\n",
       "  'Florian',\n",
       "  'Beqiri',\n",
       "  'Festim',\n",
       "  'Krasniqi',\n",
       "  'Kaonna',\n",
       "  'Krasniqi',\n",
       "  'Taulanti',\n",
       "  'Grajqevci',\n",
       "  'Arian',\n",
       "  'G',\n",
       "  'Hoti',\n",
       "  'Mehr',\n",
       "  'Beiträge',\n",
       "  'Filter',\n",
       "  'Njomza',\n",
       "  'Krasniqi',\n",
       "  'hat',\n",
       "  'ihr',\n",
       "  'Titelbild',\n",
       "  'aktualisiert',\n",
       "  '.',\n",
       "  '4',\n",
       "  '.',\n",
       "  'April',\n",
       "  '2013',\n",
       "  '·',\n",
       "  'Teilen',\n",
       "  '\\xa0',\n",
       "  '1'],\n",
       " 'labels': [1,\n",
       "  17,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  16,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  11,\n",
       "  9,\n",
       "  11,\n",
       "  9,\n",
       "  11,\n",
       "  9,\n",
       "  11,\n",
       "  9,\n",
       "  11,\n",
       "  9,\n",
       "  9,\n",
       "  11,\n",
       "  9,\n",
       "  11,\n",
       "  9,\n",
       "  11,\n",
       "  9,\n",
       "  11,\n",
       "  9,\n",
       "  9,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = dataset[\"train\"].features[f\"labels\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/deepset/gbert-large/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/ace1801aadb4eaa6eddd4240e19842e80bf7ed5859519382b3095d9e9de08ff1.4d65bbd3b91f2762e9d2c779d48ab14052439d3fcc8c3d2fe78c7322a9ac8d64\n",
      "loading file https://huggingface.co/deepset/gbert-large/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/deepset/gbert-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/deepset/gbert-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/deepset/gbert-large/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/52a68b9ec5a157f2334ff64e6e5d1d49db0f49c1a16a21e74f497d67599ca4d8.bd85f6a0d9c2848260fdda7056a557a7fa213a57b5ed27b530abd20572c0cbf4\n",
      "loading configuration file https://huggingface.co/deepset/gbert-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/71ba16830003a6d7e59484ee6aa7eb7c9e363b6b7be80c0d1013e2475cb1383e.d84aeb4b7142b5ef20ccb0f0de31fd9a1266cc177e389c7a6eaf0e72121b4ff6\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-large\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/deepset/gbert-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/71ba16830003a6d7e59484ee6aa7eb7c9e363b6b7be80c0d1013e2475cb1383e.d84aeb4b7142b5ef20ccb0f0de31fd9a1266cc177e389c7a6eaf0e72121b4ff6\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-large\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/deepset/gbert-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/71ba16830003a6d7e59484ee6aa7eb7c9e363b6b7be80c0d1013e2475cb1383e.d84aeb4b7142b5ef20ccb0f0de31fd9a1266cc177e389c7a6eaf0e72121b4ff6\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/deepset/gbert-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/22f7190d36dd7dc8f243844ef71edd605465fb99524c136f825a337f73a61434.a6c149382507da8bec5b92128f31b6859b7190394171787aedead3c386471d16\n",
      "Some weights of the model checkpoint at deepset/gbert-large were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at deepset/gbert-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Load Model and Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('deepset/gbert-large')\n",
    "model = BertForTokenClassification.from_pretrained('deepset/gbert-large', num_labels=len(label_names) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    #print(examples.keys())\n",
    "    tokenized_inputs = tokenizer(examples[\"texts\"],max_length= 512, truncation=True, is_split_into_words=True) # set max length 1024\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b0dafb3016452189a71067dfbba4b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e3de11ceac445290e41c2eef7c6fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['texts', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 24\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['texts', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset = tokenized_datasets[\"train\"]\n",
    "full_eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    for k in results.keys():\n",
    "      if(k not in flattened_results.keys()):\n",
    "        flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
    "\n",
    "    return flattened_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=6,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_args, train_dataset=full_train_dataset, eval_dataset=full_eval_dataset,compute_metrics=compute_metrics, data_collator=data_collator,\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running training *****\n",
      "  Num examples = 24\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 6\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 03:05, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>Location F1</th>\n",
       "      <th>Location City F1</th>\n",
       "      <th>Location Country F1</th>\n",
       "      <th>Misc Comments F1</th>\n",
       "      <th>Misc Friends F1</th>\n",
       "      <th>Misc Likes F1</th>\n",
       "      <th>Misc Post F1</th>\n",
       "      <th>Organization F1</th>\n",
       "      <th>Person Firstname F1</th>\n",
       "      <th>Person Lastname F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.686507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.883264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.669551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.883264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.582577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.883264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.548154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.883264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.533057</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.114754</td>\n",
       "      <td>0.880286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.505236</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.892793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.648452</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.777844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.424186</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.017094</td>\n",
       "      <td>0.894580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.472764</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.887433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.465734</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.123894</td>\n",
       "      <td>0.849315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.427561</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>0.178862</td>\n",
       "      <td>0.898154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.471946</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.281690</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.428315</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>0.408451</td>\n",
       "      <td>0.414286</td>\n",
       "      <td>0.898154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.438214</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.323944</td>\n",
       "      <td>0.282209</td>\n",
       "      <td>0.887433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.428698</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.366197</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.867183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.500808</td>\n",
       "      <td>0.289720</td>\n",
       "      <td>0.436620</td>\n",
       "      <td>0.348315</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.459016</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.481454</td>\n",
       "      <td>0.391753</td>\n",
       "      <td>0.535211</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.882668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.566153</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.535211</td>\n",
       "      <td>0.429379</td>\n",
       "      <td>0.867183</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.555936</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.480447</td>\n",
       "      <td>0.865396</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.528197</td>\n",
       "      <td>0.417582</td>\n",
       "      <td>0.535211</td>\n",
       "      <td>0.469136</td>\n",
       "      <td>0.873734</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.642489</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.455959</td>\n",
       "      <td>0.848124</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.538606</td>\n",
       "      <td>0.440476</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.477419</td>\n",
       "      <td>0.886242</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.489718</td>\n",
       "      <td>0.347458</td>\n",
       "      <td>0.577465</td>\n",
       "      <td>0.433862</td>\n",
       "      <td>0.888624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.449499</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.591549</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.900536</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.619756</td>\n",
       "      <td>0.336066</td>\n",
       "      <td>0.577465</td>\n",
       "      <td>0.424870</td>\n",
       "      <td>0.852889</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.651212</td>\n",
       "      <td>0.383929</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.469945</td>\n",
       "      <td>0.850506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.616459</td>\n",
       "      <td>0.435185</td>\n",
       "      <td>0.661972</td>\n",
       "      <td>0.525140</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.690909</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.669288</td>\n",
       "      <td>0.443396</td>\n",
       "      <td>0.661972</td>\n",
       "      <td>0.531073</td>\n",
       "      <td>0.860036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.690909</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.785142</td>\n",
       "      <td>0.354331</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.832638</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.816808</td>\n",
       "      <td>0.349206</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.446701</td>\n",
       "      <td>0.834425</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.734013</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.432749</td>\n",
       "      <td>0.853484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.769321</td>\n",
       "      <td>0.364407</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.455026</td>\n",
       "      <td>0.836212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.685125</td>\n",
       "      <td>0.357724</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0.867183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.631172</td>\n",
       "      <td>0.376068</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.886242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.652497</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.502793</td>\n",
       "      <td>0.879095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.756540</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.502732</td>\n",
       "      <td>0.860631</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.870846</td>\n",
       "      <td>0.393443</td>\n",
       "      <td>0.676056</td>\n",
       "      <td>0.497409</td>\n",
       "      <td>0.840977</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.937097</td>\n",
       "      <td>0.335821</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.829661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914375</td>\n",
       "      <td>0.335821</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.836212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.910302</td>\n",
       "      <td>0.314685</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.834425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.942445</td>\n",
       "      <td>0.313869</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.413462</td>\n",
       "      <td>0.829065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.561404</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915368</td>\n",
       "      <td>0.344262</td>\n",
       "      <td>0.591549</td>\n",
       "      <td>0.435233</td>\n",
       "      <td>0.839190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.596491</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.974196</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.821322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.952259</td>\n",
       "      <td>0.343511</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.445545</td>\n",
       "      <td>0.825491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.889723</td>\n",
       "      <td>0.390909</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.475138</td>\n",
       "      <td>0.840381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.981731</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.591549</td>\n",
       "      <td>0.439791</td>\n",
       "      <td>0.815366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.987628</td>\n",
       "      <td>0.356589</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.812388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.871881</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.476684</td>\n",
       "      <td>0.833234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.784506</td>\n",
       "      <td>0.412844</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.870756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.771995</td>\n",
       "      <td>0.424528</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.875521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.789646</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.497238</td>\n",
       "      <td>0.868970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.834427</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.502793</td>\n",
       "      <td>0.863014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.861810</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.465608</td>\n",
       "      <td>0.854080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.868003</td>\n",
       "      <td>0.369748</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>0.852293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.861533</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.475676</td>\n",
       "      <td>0.855867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.854720</td>\n",
       "      <td>0.389381</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.857653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.848443</td>\n",
       "      <td>0.389381</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.859440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.874040</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.661972</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.857653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.876521</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.529101</td>\n",
       "      <td>0.859440</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.854083</td>\n",
       "      <td>0.431193</td>\n",
       "      <td>0.661972</td>\n",
       "      <td>0.522222</td>\n",
       "      <td>0.863014</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.839687</td>\n",
       "      <td>0.439252</td>\n",
       "      <td>0.661972</td>\n",
       "      <td>0.528090</td>\n",
       "      <td>0.865992</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.867679</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.502732</td>\n",
       "      <td>0.857653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.924320</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.471795</td>\n",
       "      <td>0.842764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.999613</td>\n",
       "      <td>0.373134</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.932588</td>\n",
       "      <td>0.379032</td>\n",
       "      <td>0.661972</td>\n",
       "      <td>0.482051</td>\n",
       "      <td>0.829661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.886175</td>\n",
       "      <td>0.396396</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.483516</td>\n",
       "      <td>0.842168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913760</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.475676</td>\n",
       "      <td>0.837403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.954213</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.676056</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.831447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.967174</td>\n",
       "      <td>0.401639</td>\n",
       "      <td>0.690141</td>\n",
       "      <td>0.507772</td>\n",
       "      <td>0.832043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.942390</td>\n",
       "      <td>0.433628</td>\n",
       "      <td>0.690141</td>\n",
       "      <td>0.532609</td>\n",
       "      <td>0.845146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912970</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.508287</td>\n",
       "      <td>0.857653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.895799</td>\n",
       "      <td>0.429907</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.516854</td>\n",
       "      <td>0.861823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.888134</td>\n",
       "      <td>0.451923</td>\n",
       "      <td>0.661972</td>\n",
       "      <td>0.537143</td>\n",
       "      <td>0.864800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.892897</td>\n",
       "      <td>0.427273</td>\n",
       "      <td>0.661972</td>\n",
       "      <td>0.519337</td>\n",
       "      <td>0.862418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.907095</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.676056</td>\n",
       "      <td>0.518919</td>\n",
       "      <td>0.859440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915302</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.676056</td>\n",
       "      <td>0.518919</td>\n",
       "      <td>0.857058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916621</td>\n",
       "      <td>0.433628</td>\n",
       "      <td>0.690141</td>\n",
       "      <td>0.532609</td>\n",
       "      <td>0.855271</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.916632</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.690141</td>\n",
       "      <td>0.535519</td>\n",
       "      <td>0.852293</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915676</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.676056</td>\n",
       "      <td>0.530387</td>\n",
       "      <td>0.851102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914968</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.519774</td>\n",
       "      <td>0.849315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915615</td>\n",
       "      <td>0.429907</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.516854</td>\n",
       "      <td>0.849911</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.917199</td>\n",
       "      <td>0.403670</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.850506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.919239</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.491620</td>\n",
       "      <td>0.851697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913740</td>\n",
       "      <td>0.419048</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.855271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911839</td>\n",
       "      <td>0.419048</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.858249</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911729</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.508671</td>\n",
       "      <td>0.858845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.935063</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.497175</td>\n",
       "      <td>0.851697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.010684</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.478723</td>\n",
       "      <td>0.825491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.065305</td>\n",
       "      <td>0.387931</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.481283</td>\n",
       "      <td>0.815366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.095893</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.486772</td>\n",
       "      <td>0.814175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.110429</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.486772</td>\n",
       "      <td>0.814175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.114045</td>\n",
       "      <td>0.386555</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.484211</td>\n",
       "      <td>0.813580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.124553</td>\n",
       "      <td>0.386555</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.484211</td>\n",
       "      <td>0.812984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.082254</td>\n",
       "      <td>0.396552</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.491979</td>\n",
       "      <td>0.814771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.056442</td>\n",
       "      <td>0.396552</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.491979</td>\n",
       "      <td>0.814771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.043452</td>\n",
       "      <td>0.396552</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.491979</td>\n",
       "      <td>0.816557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.035968</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.486772</td>\n",
       "      <td>0.817153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.030908</td>\n",
       "      <td>0.393162</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.817749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.027355</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.481675</td>\n",
       "      <td>0.820727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.026112</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.481675</td>\n",
       "      <td>0.820727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.08971442222595215, metrics={'train_runtime': 186.2316, 'train_samples_per_second': 12.887, 'train_steps_per_second': 2.148, 'total_flos': 2135282114956068.0, 'train_loss': 0.08971442222595215, 'epoch': 100.0})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: texts.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6682612299919128,\n",
       " 'eval_overall_precision': 0.38461538461538464,\n",
       " 'eval_overall_recall': 0.6395348837209303,\n",
       " 'eval_overall_f1': 0.480349344978166,\n",
       " 'eval_overall_accuracy': 0.8915917503966155,\n",
       " 'eval_LOCATION_f1': 0.0,\n",
       " 'eval_LOCATION_ASSOSIATION_f1': 0.0,\n",
       " 'eval_LOCATION_CITY_f1': 0.631578947368421,\n",
       " 'eval_LOCATION_COUNTRY_f1': 0.0,\n",
       " 'eval_MISC_COMMENTS_f1': 0.14814814814814814,\n",
       " 'eval_MISC_FRIENDS_f1': 0.627906976744186,\n",
       " 'eval_MISC_LIKES_f1': 0.5517241379310345,\n",
       " 'eval_MISC_POST_f1': 0.0,\n",
       " 'eval_ORGANIZATION_f1': 0.0,\n",
       " 'eval_PERSON_FIRSTNAME_f1': 0.923076923076923,\n",
       " 'eval_PERSON_LASTNAME_f1': 1.0,\n",
       " 'eval_PERSON_NAME_f1': 0.0,\n",
       " 'eval_runtime': 0.1664,\n",
       " 'eval_samples_per_second': 36.065,\n",
       " 'eval_steps_per_second': 6.011,\n",
       " 'epoch': 100.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./NEWS_NER_2/config.json\n",
      "Model weights saved in ./NEWS_NER_2/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('./NEWS_NER_2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./NEWS_TOK_2/tokenizer_config.json\n",
      "Special tokens file saved in ./NEWS_TOK_2/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./NEWS_TOK_2/tokenizer_config.json',\n",
       " './NEWS_TOK_2/special_tokens_map.json',\n",
       " './NEWS_TOK_2/vocab.txt',\n",
       " './NEWS_TOK_2/added_tokens.json',\n",
       " './NEWS_TOK_2/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./NEWS_TOK_2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.11371712386608124,\n",
       " 'eval_overall_precision': 0.8902077151335311,\n",
       " 'eval_overall_recall': 0.9433962264150944,\n",
       " 'eval_overall_f1': 0.916030534351145,\n",
       " 'eval_overall_accuracy': 0.982909090909091,\n",
       " 'eval_LOCATION_f1': 0.2727272727272727,\n",
       " 'eval_LOCATION_ASSOSIATION_f1': 0.6666666666666666,\n",
       " 'eval_LOCATION_CITY_f1': 0.8823529411764706,\n",
       " 'eval_LOCATION_COUNTRY_f1': 0.5,\n",
       " 'eval_LOCATION_STREET_f1': 1.0,\n",
       " 'eval_ORGANIZATION_f1': 0.6086956521739131,\n",
       " 'eval_PERSON_AGE_f1': 1.0,\n",
       " 'eval_PERSON_FIRSTNAME_f1': 1.0,\n",
       " 'eval_PERSON_LASTNAME_f1': 0.9600000000000001,\n",
       " 'eval_SENTENCE_END_f1': 0.9822064056939502,\n",
       " 'eval_runtime': 0.2295,\n",
       " 'eval_samples_per_second': 43.569,\n",
       " 'eval_steps_per_second': 8.714,\n",
       " 'epoch': 100.0}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'eval_loss': 0.6682612299919128,\n",
    " 'eval_overall_precision': 0.38461538461538464,\n",
    " 'eval_overall_recall': 0.6395348837209303,\n",
    " 'eval_overall_f1': 0.480349344978166,\n",
    " 'eval_overall_accuracy': 0.8915917503966155,\n",
    " 'eval_LOCATION_f1': 0.0,\n",
    " 'eval_LOCATION_ASSOSIATION_f1': 0.0,\n",
    " 'eval_LOCATION_CITY_f1': 0.631578947368421,\n",
    " 'eval_LOCATION_COUNTRY_f1': 0.0,\n",
    " 'eval_MISC_COMMENTS_f1': 0.14814814814814814,\n",
    " 'eval_MISC_FRIENDS_f1': 0.627906976744186,\n",
    " 'eval_MISC_LIKES_f1': 0.5517241379310345,\n",
    " 'eval_MISC_POST_f1': 0.0,\n",
    " 'eval_ORGANIZATION_f1': 0.0,\n",
    " 'eval_PERSON_FIRSTNAME_f1': 0.923076923076923,\n",
    " 'eval_PERSON_LASTNAME_f1': 1.0,\n",
    " 'eval_PERSON_NAME_f1': 0.0,\n",
    " 'eval_runtime': 0.1664,\n",
    " 'eval_samples_per_second': 36.065,\n",
    " 'eval_steps_per_second': 6.011,\n",
    " 'epoch': 100.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
